# [IE-534 / CS-547 Deep Learning](https://courses.engr.illinois.edu/ie534/fa2019/)

[![Open Source Love](https://badges.frapsoft.com/os/mit/mit.svg?v=102)](https://github.com/ellerbrock/open-source-badge/)
[![HitCount](http://hits.dwyl.com/guptakhil12/Deep-Learning-UIUC.svg)](http://hits.dwyl.com/guptakhil12/Deep-Learning-UIUC)

## Table of Contents:

* [Course Information](#course-information)
* [Pre-requisites](#pre-requisites)
* [Grading](#grading)
* [Instructors](#instructors)
* [Homeworks](#homeworks)

## Course Information:

### Overview:
This course is an introduction to deep learning. Topics include convolution neural networks, recurrent neural networks, and deep reinforcement learning. <br>Homeworks on image classification, video recognition, and deep reinforcement learning. <br>Training of deep learning models using PyTorch. <br>A large amount of GPU resources are provided to the class.
<br>Mathematical analysis of neural networks, reinforcement learning, and stochastic gradient descent algorithms will also be covered in lectures.

### Topics:
- Fully-connected and feedforward networks
- Convolution networks
- Backpropagation 
- Stochastic Gradient Descent
- Hyperparameter selection and parameter initialization
- Optimization algorithms (RMSprop, ADAM, momentum, etc.)
- Second-order optimization (e.g., Hessian-free optimization)
- TensorFlow, PyTorch, automatic differentiation, static versus dynamic graphs, define-by-run
- Regularization (L2 penalty, dropout, ensembles, data augmentation techniques)
- Batch normalization
- Residual neural networks
- Recurrent neural networks (LSTM and GRU networks)
- Video recognition (two-stream convolution network, 3D convolution networks, convolution networks combined with LSTM, optical flow)
- Generative Adversarial Networks
- Deep reinforcement learning (Q-learning, actor-critic, policy gradient, experience replay, double Q-learning, deep bootstrap networks, generalized advantage estimation, dueling network, continuous control, Atari games, AlphaGo)
- Distributed training of deep learning models (e.g., asynchronous stochastic gradient descent)
- Theory of deep learning (universal approximation theorem, convergence rate, and recent mathematical results)
- Convergence analysis of stochastic gradient descent, policy gradient, tabular Q-learning

## Pre-requisites

CS 446 (or equivalent). Python. Basic statistics, probability, and optimization. Basic knowledge of Bash/Linux is recommended.

## Grading

- 35% Homeworks
- 35% Midterm
- 30% Final Project

## Instructors & TA's

- Justin Sirignano, Website [[Link]](https://jasirign.github.io/)

* Yuanyi Zhong
* Xiaobo Dong
* Lei Fan
* Rachneet Kaur
* Jyoti Aneja
* Peijun Xiao 

All copyrights reserved Â© IE534 Instructors & TAs

## Homeworks
- <a href="https://github.com/guptakhil12/CS-547-IE-534-Deep-Learning-UIUC/tree/master/homework/HW1">HW 1</a> : Train a single-layer neural network from scratch in Python using NumPy for MNIST dataset.
- <a href="https://github.com/guptakhil12/CS-547-IE-534-Deep-Learning-UIUC/tree/master/homework/HW2">HW 2</a> : Train a conolutional neural network with multiple channels in Python using NumPy for MNIST dataset.
